{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53ce4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import jit\n",
    "import pickle\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "256b3899",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "torch.from_numpy(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "540b3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch_geometric.nn as gnn\n",
    "import torch_geometric.data as gd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SignalGCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SignalGCN, self).__init__()\n",
    "        self.conv1 = gnn.GCNConv(1, 256)\n",
    "        self.pool1 = gnn.global_mean_pool()\n",
    "        self.conv2 = gnn.GCNConv(128,64)\n",
    "        self.pool2 = gnn.global_mean_pool()\n",
    "        #self.fc1 = nn.Linear(32, 16)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.d1 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(16, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create graph from 1D signal\n",
    "        data = encode_signal(x)\n",
    "\n",
    "        # Pass through GCN\n",
    "        print(data.x.shape)\n",
    "        x = F.relu(self.conv1(data.x, data.edge_index))\n",
    "        print(\"First conv\", x.shape)\n",
    "        x = self.pool1(x, batch)\n",
    "        print(\"First pool\", x.shape)\n",
    "        #x = x.view(-1, 32, 128)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x, data.edge_index))\n",
    "        x = self.pool2(x)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        print(\"final pool\", x.shape)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.d1(x)\n",
    "        x = self.fc3(x)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "f817c55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[-7.7464e-05,  1.3985e-03,  1.7646e-03,  1.9350e-03,  2.0490e-03,\n",
       "           2.1343e-03,  2.2026e-03,  2.2598e-03,  2.3090e-03,  2.3519e-03,\n",
       "           2.3897e-03,  2.4231e-03,  2.4528e-03,  2.4791e-03,  2.5026e-03,\n",
       "           2.5236e-03,  2.5423e-03,  2.5590e-03,  2.5740e-03,  2.5875e-03,\n",
       "           2.5995e-03,  2.6104e-03,  2.6201e-03,  2.6288e-03,  2.6367e-03,\n",
       "           2.6438e-03,  2.6502e-03,  2.6559e-03,  2.6611e-03,  2.6657e-03,\n",
       "           2.6699e-03,  2.6737e-03,  2.6771e-03,  2.6801e-03,  2.6829e-03,\n",
       "           2.6854e-03,  2.6876e-03,  2.6897e-03,  2.6915e-03,  2.6931e-03,\n",
       "           2.6946e-03,  2.6960e-03,  2.6972e-03,  2.6983e-03,  2.6992e-03,\n",
       "           2.7001e-03,  2.7009e-03,  2.7017e-03,  2.7023e-03,  2.7029e-03,\n",
       "           2.7034e-03,  2.7039e-03,  2.7043e-03,  2.7047e-03,  2.7051e-03,\n",
       "           2.7054e-03,  2.7057e-03,  2.7059e-03,  2.7062e-03,  2.7064e-03,\n",
       "           2.7066e-03,  2.7067e-03,  2.7069e-03,  2.7070e-03,  2.7072e-03,\n",
       "           2.7073e-03,  2.7074e-03,  2.7075e-03,  2.7075e-03,  2.7076e-03,\n",
       "           2.7077e-03,  2.7077e-03,  2.7078e-03,  2.7079e-03,  2.7079e-03,\n",
       "           2.7079e-03,  2.7080e-03,  2.7080e-03,  2.7080e-03,  2.7081e-03,\n",
       "           2.7081e-03,  2.7081e-03,  2.7081e-03,  2.7081e-03,  2.7082e-03,\n",
       "           2.7082e-03,  2.7082e-03,  2.7082e-03,  2.7082e-03,  2.7082e-03,\n",
       "           2.7082e-03,  2.7078e-03,  1.8436e-03, -1.3018e-05, -6.4296e-05,\n",
       "          -6.5869e-05, -6.6068e-05, -6.6101e-05, -6.6106e-05, -6.6107e-05,\n",
       "          -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05,\n",
       "          -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05,\n",
       "          -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05,\n",
       "          -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05,\n",
       "          -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05, -6.6107e-05,\n",
       "          -6.6107e-05, -6.6107e-05, -6.6107e-05],\n",
       "         [ 3.7028e-06,  8.5447e-04,  2.4165e-03,  3.2911e-03,  3.8131e-03,\n",
       "           4.1616e-03,  4.4164e-03,  4.6161e-03,  4.7806e-03,  4.9210e-03,\n",
       "           5.0441e-03,  5.1540e-03,  5.2534e-03,  5.3443e-03,  5.4281e-03,\n",
       "           5.5060e-03,  5.5787e-03,  5.6470e-03,  5.7113e-03,  5.7723e-03,\n",
       "           5.8301e-03,  5.8852e-03,  5.9378e-03,  5.9882e-03,  6.0366e-03,\n",
       "           6.0831e-03,  6.1279e-03,  6.1712e-03,  6.2130e-03,  6.2534e-03,\n",
       "           6.2927e-03,  6.3307e-03,  6.3678e-03,  6.4038e-03,  6.4388e-03,\n",
       "           6.4730e-03,  6.5064e-03,  6.5390e-03,  6.5708e-03,  6.6019e-03,\n",
       "           6.6324e-03,  6.6623e-03,  6.6916e-03,  6.7203e-03,  6.7484e-03,\n",
       "           6.7761e-03,  6.8033e-03,  6.8300e-03,  6.8562e-03,  6.8820e-03,\n",
       "           6.9074e-03,  6.9323e-03,  6.9569e-03,  6.9811e-03,  7.0049e-03,\n",
       "           7.0284e-03,  7.0515e-03,  7.0743e-03,  7.0968e-03,  7.1189e-03,\n",
       "           7.1407e-03,  7.1622e-03,  7.1834e-03,  7.2044e-03,  7.2250e-03,\n",
       "           7.2453e-03,  7.2654e-03,  7.2852e-03,  7.3047e-03,  7.3239e-03,\n",
       "           7.3429e-03,  7.3616e-03,  7.3801e-03,  7.3983e-03,  7.4163e-03,\n",
       "           7.4340e-03,  7.4515e-03,  7.4688e-03,  7.4858e-03,  7.5026e-03,\n",
       "           7.5192e-03,  7.5355e-03,  7.5517e-03,  7.5676e-03,  7.5833e-03,\n",
       "           7.5988e-03,  7.6140e-03,  7.6291e-03,  7.6440e-03,  7.6587e-03,\n",
       "           7.6731e-03,  7.6874e-03,  7.2785e-03,  5.3442e-03,  4.0624e-03,\n",
       "           3.2302e-03,  2.5775e-03,  1.9912e-03,  1.4894e-03,  3.7935e-04,\n",
       "           1.3480e-04,  5.0629e-05,  1.8752e-05,  6.7464e-06,  2.2599e-06,\n",
       "           5.9030e-07, -2.9726e-08, -2.5978e-07, -3.4510e-07, -3.7673e-07,\n",
       "          -3.8846e-07, -3.9281e-07, -3.9442e-07, -3.9502e-07, -3.9524e-07,\n",
       "          -3.9533e-07, -3.9536e-07, -3.9537e-07, -3.9537e-07, -3.9537e-07,\n",
       "          -3.9537e-07, -3.9537e-07, -3.9537e-07, -3.9537e-07, -3.9537e-07,\n",
       "          -3.9537e-07, -3.9537e-07, -3.9537e-07]]),\n",
       " 'output': tensor([1.0432e+04, 1.2979e+04, 4.8232e-01, 4.8024e-01, 3.8375e-10, 7.3143e-10])}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f64175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_signal(signal):\n",
    "    # Construct graph from signal\n",
    "    batch_size = signal.shape[0]\n",
    "    \n",
    "    graphs = []\n",
    "    for i in range(batch_size):\n",
    "        # For simplicity, just create a fully connected graph\n",
    "        edge_index = torch.tensor([range(len(signal)-1), range(1, len(signal))]).long()\n",
    "        x = signal[i, 0].reshape(128, 1)\n",
    "        data = gd.Data(x=x, edge_index=edge_index)\n",
    "        graphs.append(data)\n",
    "    \n",
    "    batch = torch_geometric.data.batch.Batch.from_data_list(graphs)\n",
    "    batch.to(DEVICE)\n",
    "    \n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791b4d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, LR=0.5, epochs=3000, val_loader=None):\n",
    "    net.to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "    criterion = nn.MSELoss()\n",
    "    all_MSE = nn.MSELoss()\n",
    "    val_losses = []\n",
    "    print(f\"Using: {DEVICE}\")\n",
    "                            \n",
    "    parameter_loss = []\n",
    "    losses = []\n",
    "    processed = 0\n",
    "    last_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        \n",
    "        net.train()\n",
    "        with tqdm(train_loader, unit=\"batch\") as it:\n",
    "            if epoch > 0:\n",
    "                it.set_postfix(lastLoss=last_loss, valLoss=val_losses[-1])\n",
    "            for idx, data in enumerate(it):\n",
    "                it.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                inp, out = data['input'].to(DEVICE), data['output'].to(DEVICE)\n",
    "\n",
    "                predicted = net(inp)\n",
    "                cost = criterion(out, predicted)\n",
    "                loss += cost.item() #* inp.size(0)\n",
    "                #print(f\"Predicted: {list(predicted)}\\nActual: {list(out)}\")\n",
    "        \n",
    "        if val_loader:\n",
    "            val_loss = 0\n",
    "            net.eval()\n",
    "            for idx, data in enumerate(val_loader):\n",
    "                inp, out = data['input'].to(DEVICE), data['output'].to(DEVICE)\n",
    "\n",
    "                predicted = net(inp)\n",
    "                cost = criterion(out, predicted)\n",
    "                val_loss += cost.item() * inp.size(0)\n",
    "            val_loss /= len(val_loader)  \n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        last_loss = loss/len(it)\n",
    "    print(\"Parameters: Skin YM, Adipose YM, Skin PR, Adipose PR, Skin Perm, Adipose Perm\")\n",
    "    print(f\"Sampled Ranges: 10e3 - 50e3, 1e3 - 25e3, 0.48 - 0.499, 0.48 - 0.499, 10e - 12-10e10, 10e-12 - 10e10\") \n",
    "    #print(f\"Average parameter loss: {np.mean(np.reshape(np.array(parameter_loss), (-1, 6)), axis=0)}\")        \n",
    "    print(f\"Average overall loss: {np.sum(losses)/epochs}\")\n",
    "    return losses, parameter_loss, val_losses\n",
    "\n",
    "def test(test_loader, net):\n",
    "    net.to(DEVICE)\n",
    "    net.eval()\n",
    "    criterion = nn.L1Loss()\n",
    "    crit = nn.L1Loss()\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "            loss = 0\n",
    "            with tqdm(test_loader, unit=\" batch\") as it:\n",
    "                for idx, data in enumerate(it):\n",
    "                    inp, out = data['input'].to(DEVICE), data['output'].to(DEVICE)\n",
    "                    \n",
    "                    predicted = net(inp)\n",
    "                    cost = criterion(out, predicted)\n",
    "                    loss += cost.item() * inp.size(0)\n",
    "                    \n",
    "                    #print(f\"\\n\\n\\nBatch: {idx}\")\n",
    "                   # print(f\"loss: {l_t}\")\n",
    "                    #for i, target in enumerate(out):\n",
    "                   #     errs = []\n",
    "                   #     print(f\"Targer: {target}, \\npredicted: {predicted[i]}\\n\\n\")\n",
    "                   #     for j in range(len(predicted)):\n",
    "                   #         errs.append(abs(predicted[i]-target[i])**2)\n",
    "                   #     print(f\"MSE: {np.mean(errs[0])}\")\n",
    "            \n",
    "            print(f\"Average Loss: {loss/len(test_loader)}\") \n",
    "            \n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188bf6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for data in train_loader:\n",
    "        graph, target = data\n",
    "        graph, target = graph.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(graph)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * graph.num_graphs\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(model, loss_fn, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            graph, target = data\n",
    "            graph, target = graph.to(device), target.to(device)\n",
    "            output = model(graph)\n",
    "            loss = loss_fn(output, target)\n",
    "            test_loss += loss.item() * graph.num_graphs\n",
    "    return test_loss / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9708cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"filtered.pkl\", \"rb\") as f:\n",
    "    runs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514f1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import tensor\n",
    "\n",
    "# Folder name will correspond to index of sample\n",
    "class Data(Dataset):\n",
    "    def __init__(self, signalFolder, sampleFile, runs, steps=128):\n",
    "        # Load both disp1 and disp2 from each folder\n",
    "        # Folders ordered according to index of sample\n",
    "        # Use the corresponding sample as y -> append probe?\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        with open(f\"{sampleFile}\", \"rb\") as f:\n",
    "             samples = pickle.load(f)\n",
    "\n",
    "        self.min = [min(np.array(samples[runs])[:, i]) for i in range(6)]\n",
    "        self.max = [max(np.array(samples[runs])[:, i]) for i in range(6)]\n",
    "        \n",
    "        for run in runs: #os.listdir(f\"{signalFolder}/\"):  \n",
    "            inp = []\n",
    "            fail = False\n",
    "            \n",
    "            files = os.listdir(f\"{signalFolder}/{run}/\")\n",
    "            \n",
    "            if files != ['Disp1.csv', 'Disp2.csv']:\n",
    "                continue\n",
    "            \n",
    "            for file in files:\n",
    "                a = pd.read_csv(f\"{signalFolder}/{run}/{file}\")\n",
    "                a.rename(columns = {'0':'x', '0.1': 'y'}, inplace = True)\n",
    "                \n",
    "                if a['x'].max() != 7.0:\n",
    "                    fail = True\n",
    "                    break\n",
    "                #a = pd.concat([pd.DataFrame([[0,0.1]], columns=a.columns), a], ignore_index=True)\n",
    "                \n",
    "\n",
    "                # Interpolate curve for consistent x values\n",
    "                xNew = np.linspace(0, 7, num=steps, endpoint=False)\n",
    "                interped = interp1d(a['x'], a['y'], kind='cubic', fill_value=\"extrapolate\")(xNew)\n",
    "                    \n",
    "                #fix, ax = plt.subplots(1, 2)\n",
    "                #ax[0].plot(xNew, interped)\n",
    "                #a.plot(ax=ax[1], x='x', y='y')\n",
    "                #if run == 3:\n",
    "                   # break\n",
    "                \n",
    "                \n",
    "                #if len(a) < 702:\n",
    "                #    print(f\"{signalFolder}/{run}/{file}: {len(a)}\")\n",
    "               #     fail = True\n",
    "                 #   break\n",
    "                \n",
    "               # while len(a) > 702:\n",
    "                #    a = a.drop(index=np.random.randint(0, len(a)-1)).reset_index(drop=True)\n",
    "                \n",
    "                #print(a)\n",
    "                \n",
    "                inp.append(list(interped))\n",
    "            \n",
    "            if not fail:\n",
    "                if len(inp) != 2:\n",
    "                    raise Exception(\"sdf\")\n",
    "                #print(inp[0])\n",
    "                #raise Exception(\"sdf\")\n",
    "\n",
    "                self.input.append(inp)\n",
    "                self.output.append(samples[int(run)])\n",
    "        \n",
    "        self.output = tensor(self.output)\n",
    "        self.input = tensor(self.input)\n",
    "        #for dim in range(6):\n",
    "       #     self.output[:, dim] = (self.output[:, dim] - self.min[dim])/(self.max[dim] - self.min[dim])\n",
    "        #print(self.output)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"input\": self.input[idx], \"output\": self.output[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb0f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "def custom_collate(data_list):\n",
    "    # Stack the `Data` objects into a `Batch` object\n",
    "    #data_list = [data_list[i] for i in range(len(data_list))]\n",
    "    print(data_list[0])\n",
    "    batch = Batch.from_data_list(data_list[0])\n",
    "    \n",
    "    batch = {\"input\": batch, \"output\": data_list[1]}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33ecc75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Data(\"D:/SamplingResults\", \"C:/Users/rjsou/Documents/MastersDiss/Sampling/Samples.pkl\", runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdd8aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_n = int(0.8 * len(dataset))\n",
    "test_n = len(dataset) - train_n\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_n, test_n])\n",
    "train_loader, test_loader = DataLoader(train_set, batch_size=2, shuffle=True, pin_memory=True), DataLoader(test_set, batch_size=2, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42fdc260",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch_geometric.nn' has no attribute 'GlobalMeanPool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mSignalGCN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mSignalGCN.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28msuper\u001b[39m(SignalGCN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m gnn\u001b[38;5;241m.\u001b[39mGCNConv(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1 \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobalMeanPool\u001b[49m()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m gnn\u001b[38;5;241m.\u001b[39mGCNConv(\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2 \u001b[38;5;241m=\u001b[39m gnn\u001b[38;5;241m.\u001b[39mGlobalMeanPool()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch_geometric.nn' has no attribute 'GlobalMeanPool'"
     ]
    }
   ],
   "source": [
    "net = SignalGCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc05059",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, net, val_loader = test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78f89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, LR=0.1, epochs=2000, val_loader=None):\n",
    "    net.to(DEVICE)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "    criterion = nn.L1Loss()\n",
    "    all_MSE = nn.L1Loss()\n",
    "    val_losses = []\n",
    "    print(f\"Using: {DEVICE}\")\n",
    "                            \n",
    "    parameter_loss = []\n",
    "    losses = []\n",
    "    processed = 0\n",
    "    last_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        net.train()\n",
    "        with tqdm(train_loader, unit=\"batch\") as it:\n",
    "            if epoch > 0:\n",
    "                it.set_postfix(lastLoss=last_loss, valLoss=val_losses[-1])\n",
    "            for idx, data in enumerate(it):\n",
    "                it.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                inp, out = data['input'].to(DEVICE), data['output'].to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predicted = net(inp)\n",
    "\n",
    "                cost = criterion(out, predicted)\n",
    "                loss += cost.item()\n",
    "                cost.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                for i in range(len(predicted)):\n",
    "                    current_MSE = []\n",
    "                    for j in range(6):\n",
    "                        current_MSE.append(all_MSE(out[i][j], predicted[i][j]).item())\n",
    "                    parameter_loss.append(current_MSE)\n",
    "                    processed += 1\n",
    "        \n",
    "        if val_loader:\n",
    "            val_loss = 0\n",
    "            net.eval()\n",
    "            for idx, data in enumerate(val_loader):\n",
    "                inp, out = data['input'].to(DEVICE), data['output'].to(DEVICE)\n",
    "\n",
    "                predicted = net(inp)\n",
    "                cost = criterion(out, predicted)\n",
    "                val_loss += cost.item()\n",
    "            val_loss /= len(val_loader)  \n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        loss /= len(it)\n",
    "        losses.append(loss)\n",
    "        last_loss = loss\n",
    "    print(\"Parameters: Skin YM, Adipose YM, Skin PR, Adipose PR, Skin Perm, Adipose Perm\")\n",
    "    print(f\"Sampled Ranges: 10e3 - 50e3, 1e3 - 25e3, 0.48 - 0.499, 0.48 - 0.499, 10e - 12-10e10, 10e-12 - 10e10\") \n",
    "    print(f\"Average parameter loss: {np.mean(np.reshape(np.array(parameter_loss), (-1, 6)), axis=0)}\")        \n",
    "    print(f\"Average overall loss: {np.sum(losses)/processed}\")\n",
    "    return losses, parameter_loss, val_losses\n",
    "\n",
    "def test(test_loader, net):\n",
    "    net.to(DEVICE)\n",
    "    net.eval()\n",
    "    criterion = nn.L1Loss()\n",
    "    crit = nn.L1Loss()\n",
    "    differences = []\n",
    "    outs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "            loss = 0\n",
    "            with tqdm(test_loader, unit=\" batch\") as it:\n",
    "                for idx, data in enumerate(it):\n",
    "                    inp, out = data['input'].to(DEVICE), data['output'].to(DEVICE)\n",
    "                    \n",
    "                    predicted = net(inp)\n",
    "                    cost = criterion(out, predicted)\n",
    "                    l_t = cost.item()\n",
    "                    loss += l_t\n",
    "                    for i in range(len(predicted)):\n",
    "                        p = predicted[i].cpu().numpy().reshape(1, -1)\n",
    "                        o = out[i].cpu().numpy().reshape(1, -1)\n",
    "                        \n",
    "                        p = SCALER.inverse_transform(p)[0]\n",
    "                        o = SCALER.inverse_transform(o)[0]\n",
    "\n",
    "                        \n",
    "                        differences.append((np.abs(p-o)/o)*100)\n",
    "                        #print(\"D\", differences[-1])\n",
    "                        #print(F\"Predicted: {SCALER.inverse_transform(p)}\")\n",
    "                        #print(F\"Real: {SCALER.inverse_transform(o)}\")\n",
    "                        #print(f\"Difference: {abs(p - o)}\\n\\n\")\n",
    "                        #processed += 1\n",
    "                    \n",
    "                    #print(f\"\\n\\n\\nBatch: {idx}\")\n",
    "                   # print(f\"loss: {l_t}\")\n",
    "            #print(differences[0])\n",
    "            #print(np.array(differences).shape)\n",
    "            #print(f\"Average Loss: {loss/len(test_loader)}\")\n",
    "            #print(f\"Average parameter loss: {np.mean(np.array(differences), axis=0)}\") # Dataset\n",
    "    return np.mean(np.array(differences), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b276f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder name will correspond to index of sample\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "SCALER = MinMaxScaler()\n",
    "class Data(Dataset):\n",
    "    def __init__(self, signalFolder, sampleFile, runs=range(65535), steps=128):\n",
    "        # Load both disp1 and disp2 from each folder\n",
    "        # Folders ordered according to index of sample\n",
    "        # Use the corresponding sample as y -> append probe?\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        \n",
    "        with open(f\"{sampleFile}\", \"rb\") as f:\n",
    "             samples = pickle.load(f)\n",
    "        \n",
    "        self.min = np.min(samples[runs])\n",
    "        self.max = np.max(samples[runs])\n",
    "        \n",
    "        for run in tqdm(runs): #os.listdir(f\"{signalFolder}/\"):  \n",
    "            inp = []\n",
    "            fail = False\n",
    "            \n",
    "            files = os.listdir(f\"{signalFolder}/{run}/\")\n",
    "            \n",
    "            if files != ['Disp1.csv', 'Disp2.csv']:\n",
    "                continue\n",
    "            \n",
    "            for file in files:\n",
    "                a = pd.read_csv(f\"{signalFolder}/{run}/{file}\")\n",
    "                a.rename(columns = {'0':'x', '0.1': 'y'}, inplace = True)\n",
    "                \n",
    "                if a['x'].max() != 7.0:\n",
    "                    fail = True\n",
    "                    break\n",
    "                #a = pd.concat([pd.DataFrame([[0,0.1]], columns=a.columns), a], ignore_index=True)\n",
    "                \n",
    "\n",
    "                # Interpolate curve for consistent x values\n",
    "                xNew = np.linspace(0, 7, num=steps, endpoint=False)\n",
    "                interped = interp1d(a['x'], a['y'], kind='cubic', fill_value=\"extrapolate\")(xNew)\n",
    "                    \n",
    "                #fix, ax = plt.subplots(1, 2)\n",
    "                #ax[0].plot(xNew, interped)\n",
    "                #a.plot(ax=ax[1], x='x', y='y')\n",
    "                #if run == 3:\n",
    "                   # break\n",
    "                \n",
    "                \n",
    "                #if len(a) < 702:\n",
    "                #    print(f\"{signalFolder}/{run}/{file}: {len(a)}\")\n",
    "               #     fail = True\n",
    "                 #   break\n",
    "                \n",
    "               # while len(a) > 702:\n",
    "                #    a = a.drop(index=np.random.randint(0, len(a)-1)).reset_index(drop=True)\n",
    "                \n",
    "                #print(a)\n",
    "                \n",
    "                inp.append(interped.astype(\"float32\"))\n",
    "            \n",
    "            if not fail:\n",
    "                if len(inp) != 2:\n",
    "                    raise Exception(\"sdf\")\n",
    "                #print(inp[0])\n",
    "                #raise Exception(\"sdf\")\n",
    "\n",
    "                self.input.append(inp)\n",
    "                self.output.append(samples[int(run)])\n",
    "        \n",
    "        SCALER.fit(self.output)\n",
    "        self.output = SCALER.fit_transform(self.output)\n",
    "        self.output = tensor(self.output).type(torch.cuda.FloatTensor)\n",
    "        self.input = tensor(self.input).type(torch.cuda.FloatTensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\"input\": self.input[idx], \"output\": self.output[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7165c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/filtered.pkl\", \"rb\") as f:\n",
    "    runs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b689dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "SCALER = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef6899e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2241/2241 [00:37<00:00, 59.79it/s]\n",
      "C:\\Users\\rjsou\\AppData\\Local\\Temp\\ipykernel_12804\\3404371057.py:73: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  self.input = tensor(self.input).type(torch.cuda.FloatTensor)\n"
     ]
    }
   ],
   "source": [
    "dataset = Data(\"D:/SamplingResults2\", \"../Sampling/newSamples.pkl\", runs=runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b72b0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = int(0.8 * len(dataset))\n",
    "test_n = len(dataset) - train_n\n",
    "train_set, test_set = random_split(dataset, [train_n, test_n])\n",
    "train_loader, test_loader = DataLoader(train_set, batch_size=32, shuffle=True), DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77feeb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Single2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "089f0c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/500:   0%|                                                                           | 0/56 [00:01<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 2, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/500:   0%|                                                                           | 0/56 [00:03<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "pad should be at most half of kernel size, but got pad=3 and kernel_size=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tl, pl, vl \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, net, LR, epochs, val_loader)\u001b[0m\n\u001b[0;32m     21\u001b[0m inp, out \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE), data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m cost \u001b[38;5;241m=\u001b[39m criterion(out, predicted)\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mSingle2D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#x = x.reshape(batch_size, 1, 2, 128)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool3(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: pad should be at most half of kernel size, but got pad=3 and kernel_size=2"
     ]
    }
   ],
   "source": [
    "tl, pl, vl = train(train_loader, net, val_loader=test_loader, LR=0.0001, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525d9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up samples to 256 from 128\n",
    "class Single2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Single2D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(7680, 512)\n",
    "        self.d1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.d2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.d3 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.unsqueeze(1)\n",
    "        #x = x.reshape(batch_size, 1, 2, 128)\n",
    "        print(x.shape)\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "       # x = self.pool4(torch.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.d1(x)\n",
    "        \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.d2(x)\n",
    "        \n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.d3(x)\n",
    "        \n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65db27ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
